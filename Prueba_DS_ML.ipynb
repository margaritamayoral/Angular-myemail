{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNG4mtdJH/2y8he5h4aRiA9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/margaritamayoral/Angular-myemail/blob/master/Prueba_DS_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIsGI9fMTBGd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Primer caso\n",
        "Supón que tienes una población que se etiqueta en 3 casos a, b, y c. Para dicha población cuentas con 500 variables que pueden afectar su comportamiento y que pueden cambiar a los que son a y pueden pasar a b o c, o cualquiera dependiendo de los cambios de las variables. Describe que ML aplicarías y por que? Qué pruebas estadísticas usarías para comprender que el modelo es consistente. Define 3 métricas para ponderar el error.\n"
      ],
      "metadata": {
        "id": "Er7n-wcsTtpK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BWjWlfW1T_Za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WJmkN7lgULtJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eKXqDNkmofqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Preparacion de los datos\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "### Entrenamiento del modelo\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score\n",
        "\n",
        "### Division del conjunto de datos en datos de entrenamiento y datos de prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocessing, considerando que tenemos variables categoricas y numericas.\n",
        "# Esta funcion realiza el preproceso de las variables para poder despues\n",
        "# utilizarla en un pipeline junto con el entrenamiento del modelo\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Definicion de la funcion modelo a utilizar durante el entrenamiento.\n",
        "# Consiste en una  pipeline que inclye el preprocesamiento de los datos y\n",
        "# la llamada al tipo de modelo a entrenar.\n",
        "model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "## Evaluacion del modelo\n",
        "\n",
        "# Realizando las inferencias y evaluando\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Metricas\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}, F1 Score: {f1}, Cohen\\'s Kappa: {kappa}')\n",
        "\n",
        "# Cross-validation:\n",
        "cross_val_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
        "print(f'Cross-validation scores: {cross_val_scores}')\n"
      ],
      "metadata": {
        "id": "vdK2hooVUQpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Segundo caso\n",
        "\n",
        "En Wal-Mart hay diferentes fuentes de las cuales se ha obtenido un dataset con 120 campos, en el cual se incluye la variable independiente, sin embargo de los 119 restantes hay 69 variables categóricas y el resto son alfanuméricas. Explica si descartarías variables o no y el por que; supongamos que de las 69 variables categóricas 60 son significativas, como las transformarías para que algún modelo pueda ser entrenado con ellas. Los dos puntos anteriores se cumplen si la variable objetivo es continua o discreta?\n"
      ],
      "metadata": {
        "id": "Rmz4G9vhW9-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulacion de los datos\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "# Para entrenar el modelo\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, f1_score\n",
        "\n",
        "# Llamando df al dataset, donde 'target' es la variable objetivo\n",
        "X = df.drop(columns=['target'])\n",
        "y = df['target']\n",
        "\n",
        "# Como primer paso, identificamos todas las variables categoricas y alfanumericas\n",
        "# existentes dentro del dataset\n",
        "\n",
        "categorical_features = [...]  # Lista de las 69 variables categoricas\n",
        "alphanumeric_features = [...]  # Lista de las variables categoricas restantes\n",
        "\n",
        "# Funciones para preprocesar los datos y utilizar en la pipeline principal\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "# Asumiendo que las variables alfanumericas pueden ser codificadas directamente\n",
        "alphanumeric_transformer = LabelEncoder()\n",
        "\n",
        "# Haciendo el preprocesamiento usnado ColumnTransformer para construir una\n",
        "# pipeline de preprocesamiento\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', categorical_transformer, categorical_features),\n",
        "        # se incluiria una transformacion para las variables alfanumericas si es\n",
        "        # necesaria\n",
        "    ])\n",
        "\n",
        "# Transformamos los datos\n",
        "X_preprocessed = preprocessor.fit_transform(X)\n",
        "\n",
        "\n",
        "# Entrenando el modelo\n",
        "\n",
        "# Separamos el dataset en datos para training y test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Determinamos si la variable objetivo es continua o discreta y definimos el\n",
        "# modelo a entrenar en base a la observacion de la variable objetivo\n",
        "if y.dtype == 'float':\n",
        "    # Modelo de regresion\n",
        "    model = RandomForestRegressor(random_state=42)\n",
        "else:\n",
        "    # Modelo de clasificacion\n",
        "    model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Entrenamos el modelo\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Realizamos la inferencia y evaluamos\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# En el caso de un modelo de regresion\n",
        "if y.dtype == 'float':\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    print(f'MSE: {mse}, R2: {r2}')\n",
        "\n",
        "# En el caso de un modelo de clasificacion\n",
        "else:\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    print(f'Accuracy: {accuracy}, F1 Score: {f1}')\n",
        "\n",
        "# Relaizamos la Cross-validation\n",
        "cross_val_scores = cross_val_score(model, X_preprocessed, y, cv=5, scoring='accuracy' if y.dtype != 'float' else 'neg_mean_squared_error')\n",
        "print(f'Cross-validation scores: {cross_val_scores}')\n",
        "\n"
      ],
      "metadata": {
        "id": "ffoIZCciXCJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problema de ML inciso e.\n",
        "\n",
        "Si tuvieras una base de datos de más de 1000 millones de datos, pero no tienes ninguna\n",
        "herramienta de big data para procesarla, ¿Qué harías para analizar la información,\n",
        "procesar datos, generar un modelo y probarlo en un ambiente productivo?"
      ],
      "metadata": {
        "id": "rR6nLi33ohw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dada la restricción que se enfrenta con una base de datos de más de mil millones de registros y sin acceso a herramientas de big data, se puede seguir un enfoque estratégico para analizar, procesar y modelar los datos de manera eficiente.\n",
        "1. Muestreo de Datos\n",
        "Dado que procesar todo el conjunto de datos en una sola máquina no es práctico, se puede usar técnicas de muestreo para crear un subconjunto representativo de los datos:\n",
        "Muestreo Aleatorio: Extraer una muestra aleatoria del conjunto de datos. Se necesita  que la muestra sea lo suficientemente grande como para capturar los patrones subyacentes.\n",
        "Muestreo Estratificado: Si los datos tienen diferentes estratos (por ejemplo, categorías, clases), hay que asegurarse de que la muestra mantenga las proporciones de cada estrato.\n",
        "2. Preprocesamiento de Datos\n",
        "Una vez obtenida una muestra manejable, se puede preprocesar los datos usando herramientas como Pandas:\n"
      ],
      "metadata": {
        "id": "TmXAKUInqAwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import joblib\n",
        "from flask import Flask, request, jsonify\n",
        "\n",
        "# Cargar una muestra del conjunto de datos\n",
        "sample_data = pd.read_csv('sample_data.csv')\n",
        "\n",
        "# Limpieza y preprocesamiento de datos\n",
        "sample_data = sample_data.dropna()  # Manejo de valores faltantes\n",
        "sample_data = pd.get_dummies(sample_data)  # Codificación one-hot de variables categóricas\n"
      ],
      "metadata": {
        "id": "qmuAHQ7QqNUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Ingeniería de Características\n",
        "Crear nuevas características o transformar las existentes para mejorar el rendimiento del modelo:"
      ],
      "metadata": {
        "id": "mFk0Wi0WqW6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Vc81939CXDX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de creación de nuevas características\n",
        "sample_data['new_feature'] = sample_data['existing_feature1'] / sample_data['existing_feature2']\n"
      ],
      "metadata": {
        "id": "EWMdnRPeqY1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Entrenamiento del Modelo\n",
        "Seleccionar un modelo de aprendizaje automático adecuado y entrenarlo con los datos muestreados. Usar algoritmos eficientes que puedan manejar grandes conjuntos de datos cuando sea necesario:"
      ],
      "metadata": {
        "id": "wmojuXBiqj3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X = sample_data.drop('target', axis=1)\n",
        "y = sample_data['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Entrenar un Clasificador Random Forest\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluar el modelo\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f'Precisión: {accuracy}, Puntaje F1: {f1}')\n"
      ],
      "metadata": {
        "id": "EUd0cfcnqqDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Validación del Modelo\n",
        "Usar validación cruzada para asegurar que el modelo se generalice bien:"
      ],
      "metadata": {
        "id": "MLrgealDq348"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
        "print(f'Puntajes de validación cruzada: {cv_scores}')"
      ],
      "metadata": {
        "id": "Bl4VP1zEq736"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Escalamiento\n",
        "Para manejar el conjunto de datos completo de forma incremental o en bloques:\n",
        "\n",
        "Procesamiento por Lotes: Procesar los datos en lotes. Entrenar el modelo de manera incremental o usar técnicas como el aprendizaje en línea.\n",
        "Procesamiento Distribuido: Utilizar marcos como Dask o Apache Spark (si están disponibles) para paralelizar el procesamiento de datos y el entrenamiento del modelo.\n",
        "7. Pruebas y Despliegue\n",
        "Después de desarrollar el modelo, desplegarlo en un entorno de producción usando herramientas que faciliten el servicio del modelo:"
      ],
      "metadata": {
        "id": "RDeFujAXrOLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar el modelo\n",
        "joblib.dump(model, 'model.pkl')\n",
        "\n",
        "# Crear una API simple con Flask para las predicciones del modelo\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    data = request.get_json(force=True)\n",
        "    prediction = model.predict(pd.DataFrame(data))\n",
        "    return jsonify(prediction.tolist())\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(port=5000, debug=True)\n"
      ],
      "metadata": {
        "id": "CtKc6pYfrQaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Monitoreo y Reentrenamiento\n",
        "Configurar el monitoreo para rastrear el rendimiento del modelo y reentrenarlo según sea necesario:\n",
        "\n",
        "Monitoreo: Utilizar herramientas como Prometheus y Grafana para monitorear el rendimiento de la API y la precisión del modelo.\n",
        "Reentrenamiento: Reentrenar periódicamente el modelo utilizando nuevas muestras de datos para asegurar que siga siendo preciso y relevante."
      ],
      "metadata": {
        "id": "HTRzBseOrj67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. PySpark\n",
        "\n",
        "Escribe las sentencias en PySpark para seleccionar toda la información en la tabla “tabla_1”, si dicha tabla tuviera un campo llamado “campo_cat”. Escribe las sentencia para seleccionar todos los valores distintos y para contar la frecuencia de cada uno de estos. Escribe la sentencia para seleccionar los primeros 100 registros, y los últimos 100 registros. Escribe la sentencia para contar todos los registros de la tabla.\n",
        "\n"
      ],
      "metadata": {
        "id": "ak8sYpIDw9Z2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seleccionando toda la información de \"Tabla_1\"\n"
      ],
      "metadata": {
        "id": "gQhRsA0ZxLUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number\n",
        "\n",
        "\n",
        "# Crear una sesión de Spark\n",
        "spark = SparkSession.builder.appName(\"SeleccionarInformacion\").getOrCreate()\n",
        "\n",
        "# Cargar la tabla \"Tabla_1\"\n",
        "tabla_1 = spark.table(\"Tabla_1\")\n",
        "\n",
        "# Seleccionar toda la información de la tabla\n",
        "tabla_1.show()\n"
      ],
      "metadata": {
        "id": "8ByPNcHqrk4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contar la frecuencia de cada valor en \"campo_cat\"\n"
      ],
      "metadata": {
        "id": "xGCirV7txPq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if \"campo_cat\" in tabla_1.columns:\n",
        "    # Contar la frecuencia de cada valor en \"campo_cat\"\n",
        "    frecuencias = tabla_1.groupBy(\"campo_cat\").count()\n",
        "    frecuencias.show()\n"
      ],
      "metadata": {
        "id": "059VGjXSxUex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seleccionar los primeros 100 registros\n"
      ],
      "metadata": {
        "id": "yhEtF_uYxlzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar los primeros 100 registros\n",
        "primeros_100 = tabla_1.limit(100)\n",
        "primeros_100.show()\n"
      ],
      "metadata": {
        "id": "ZWlKoR0Qxp_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seleccionar los últimos 100 registros\n",
        "Para seleccionar los últimos 100 registros, primero necesitamos conocer el número total de registros y luego restar 100 para obtener los últimos registros."
      ],
      "metadata": {
        "id": "7U0863YWxuJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contar el número total de registros\n",
        "total_registros = tabla_1.count()\n",
        "\n",
        "# Crear una columna con el número de fila\n",
        "windowSpec = Window.orderBy(\"some_column\")  # Reemplaza \"some_column\" con una columna adecuada para el orden\n",
        "tabla_1_con_fila = tabla_1.withColumn(\"row_num\", row_number().over(windowSpec))\n",
        "\n",
        "# Seleccionar los últimos 100 registros\n",
        "ultimos_100 = tabla_1_con_fila.filter(tabla_1_con_fila.row_num > (total_registros - 100))\n",
        "ultimos_100.show()\n"
      ],
      "metadata": {
        "id": "BBMZgQOpx-6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contar todos los registros en la tabla\n"
      ],
      "metadata": {
        "id": "znTmyPBcyOqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contar todos los registros en la tabla\n",
        "conteo_total = tabla_1.count()\n",
        "print(f\"Total de registros en la tabla: {conteo_total}\")\n"
      ],
      "metadata": {
        "id": "j2rEQkQmyPef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SP5pbsQ10SPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark inciso b"
      ],
      "metadata": {
        "id": "2YLcY96Y0YX5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crear las tablas en PySpark\n",
        "Primero, vamos a crear las tablas Customer_info y Customer_purchs en PySpark:"
      ],
      "metadata": {
        "id": "k7iToEkz0pxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Crear una sesión de Spark\n",
        "spark = SparkSession.builder.appName(\"CustomerQueries\").getOrCreate()\n",
        "\n",
        "# Datos para Customer_info\n",
        "data_customer_info = [\n",
        "    (\"1sd\", \"A\", 1),\n",
        "    (\"2sd\", \"B\", 1),\n",
        "    (\"3sd\", \"A\", 0),\n",
        "    (\"45sd\", \"C\", 0),\n",
        "    (\"9sd\", \"C\", 1)\n",
        "]\n",
        "\n",
        "# Datos para Customer_purchs\n",
        "data_customer_purchs = [\n",
        "    (\"1sd\", \"21-09-19\", \"Aaabbb\", 3, 5.5),\n",
        "    (\"1sd\", \"21-09-19\", \"bbbccc\", 1, 80),\n",
        "    (\"1sd\", \"22-09-19\", \"Cccaaa\", 4, 17.5),\n",
        "    (\"2sd\", \"23-09-19\", \"Dddaaa\", 12, 7.5),\n",
        "    (\"2sd\", \"23-09-19\", \"Aaabbb\", 2, 5.5),\n",
        "    (\"3sd\", \"21-09-19\", \"Aaabbb\", 6, 5.5),\n",
        "    (\"3sd\", \"21-09-19\", \"bbbccc\", 6, 80)\n",
        "]\n",
        "\n",
        "# Crear DataFrames\n",
        "columns_customer_info = [\"Id_cliente\", \"City_chr\", \"Gndr_chr\"]\n",
        "columns_customer_purchs = [\"Id_cliente\", \"date_chr\", \"Prod_chr\", \"Qty_num\", \"Price_num\"]\n",
        "\n",
        "df_customer_info = spark.createDataFrame(data_customer_info, columns_customer_info)\n",
        "df_customer_purchs = spark.createDataFrame(data_customer_purchs, columns_customer_purchs)\n",
        "\n",
        "# Crear tablas temporales\n",
        "df_customer_info.createOrReplaceTempView(\"Customer_info\")\n",
        "df_customer_purchs.createOrReplaceTempView(\"Customer_purchs\")\n"
      ],
      "metadata": {
        "id": "9yOHd_xs0Rfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Género que compra más artículos, la ciudad que compró más, y el cliente que compró más\n"
      ],
      "metadata": {
        "id": "XdCGB0G20vzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_c = \"\"\"\n",
        "WITH total_purchases AS (\n",
        "    SELECT\n",
        "        cp.Id_cliente,\n",
        "        ci.Gndr_chr,\n",
        "        ci.City_chr,\n",
        "        SUM(cp.Qty_num) AS total_qty\n",
        "    FROM Customer_purchs cp\n",
        "    JOIN Customer_info ci ON cp.Id_cliente = ci.Id_cliente\n",
        "    GROUP BY cp.Id_cliente, ci.Gndr_chr, ci.City_chr\n",
        "),\n",
        "gender_purchase AS (\n",
        "    SELECT Gndr_chr, SUM(total_qty) AS total_gender_qty\n",
        "    FROM total_purchases\n",
        "    GROUP BY Gndr_chr\n",
        "),\n",
        "city_purchase AS (\n",
        "    SELECT City_chr, SUM(total_qty) AS total_city_qty\n",
        "    FROM total_purchases\n",
        "    GROUP BY City_chr\n",
        "),\n",
        "customer_purchase AS (\n",
        "    SELECT Id_cliente, SUM(total_qty) AS total_customer_qty\n",
        "    FROM total_purchases\n",
        "    GROUP BY Id_cliente\n",
        ")\n",
        "\n",
        "SELECT 'Género' AS Category, Gndr_chr AS Identifier, total_gender_qty AS Total\n",
        "FROM gender_purchase\n",
        "UNION ALL\n",
        "SELECT 'Ciudad' AS Category, City_chr AS Identifier, total_city_qty AS Total\n",
        "FROM city_purchase\n",
        "UNION ALL\n",
        "SELECT 'Cliente' AS Category, Id_cliente AS Identifier, total_customer_qty AS Total\n",
        "FROM customer_purchase\n",
        "ORDER BY Category, Total DESC\n",
        "\"\"\"\n",
        "\n",
        "result_c = spark.sql(query_c)\n",
        "result_c.show()\n"
      ],
      "metadata": {
        "id": "abAuxQsp04HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Promedio de compra por ciudad\n"
      ],
      "metadata": {
        "id": "SgW03EWh08-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_d = \"\"\"\n",
        "SELECT\n",
        "    ci.City_chr,\n",
        "    AVG(cp.Qty_num * cp.Price_num) AS avg_purchase_per_city\n",
        "FROM Customer_purchs cp\n",
        "JOIN Customer_info ci ON cp.Id_cliente = ci.Id_cliente\n",
        "GROUP BY ci.City_chr\n",
        "\"\"\"\n",
        "\n",
        "result_d = spark.sql(query_d)\n",
        "result_d.show()\n"
      ],
      "metadata": {
        "id": "bubaY1PI1BXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tabla ordenada por fecha, de compra total por cliente y ciudad\n"
      ],
      "metadata": {
        "id": "c41L4SYu1He9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_e = \"\"\"\n",
        "SELECT\n",
        "    ci.Id_cliente,\n",
        "    ci.City_chr,\n",
        "    cp.date_chr,\n",
        "    SUM(cp.Qty_num * cp.Price_num) AS total_purchase\n",
        "FROM Customer_purchs cp\n",
        "JOIN Customer_info ci ON cp.Id_cliente = ci.Id_cliente\n",
        "GROUP BY ci.Id_cliente, ci.City_chr, cp.date_chr\n",
        "ORDER BY cp.date_chr, ci.Id_cliente, ci.City_chr\n",
        "\"\"\"\n",
        "\n",
        "result_e = spark.sql(query_e)\n",
        "result_e.show()\n"
      ],
      "metadata": {
        "id": "zzUdvvvq1MQt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}